hydra:
  job:
    chdir: false
defaults:
  - model_config: hf-gen_a      # the model config of the LM inferencer, can be `hf-gen_a` or `api-gen-a`

model_name: '/data/pretrain_models/llama-7b" #EleutherAI/gpt-neo-2.7B #gpt2-large #???   #â€œ/data/pretrain_models/llama-7b"             # the model name of the LM inferencer
task_name: mrpc #???
output_file: output/epr/mrpc/gpt2-large/base-mg0.02-s0.1-fix/huggyllama_llama-7b/pred.json #???                # predictions will be saved to `output_file`

batch_size: 8 #8 for gep-neo               # the batch_size of the model when using `hf-gen_a` model_config; for api models, the batch size is decided based on the number of openai keys.

# parameters needed to initialize the inference dataset reader
dataset_reader:
  _target_: src.dataset_readers.inference_dsr.InferenceDatasetReader
  dataset_path: output/dpp-epr-random/mrpc/gpt2-large/base-mg0.02-s0.1-fix/train_retrieved.json #null          # one of `dataset_path` and `dataset_split` must be set
  dataset_split: null
  task_name: mrpc #${task_name}
  model_name: "/data/pretrain_models/llama-7b" #EleutherAI/gpt-neo-2.7B #gpt2-large #${model_name}
  n_tokens: 1600 # 700 #???                # maximum number of tokens as prompt
  field: gen_a                  # this field will be used to construct prompt
  ds_size: null                # number of instances used for the dataset, 'null' refers to 'all'
  index_reader: ${index_reader}

# parameters needed to initialize the index reader
index_reader:
  _target_: src.dataset_readers.index_dsr.IndexDatasetReader
  task_name: mrpc #${task_name}
  model_name: "/data/pretrain_models/llama-7b" # EleutherAI/gpt-neo-2.7B #gpt2-large #${model_name}
  field: qa                    # the field used for in-context examples, `qa` refers to the whole input-output pairs
  dataset_path: index_data/mrpc/index_dataset.json #null          # one of `dataset_path` and `dataset_split` must be set
  dataset_split: null
  ds_size: null